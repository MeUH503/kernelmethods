{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of kernel methods library\n",
    "\n",
    "In this notebook we present the various important components of the `kernelmethods` library, and provide some example usage scenarios.\n",
    "\n",
    "This library consists of a set of key classes such as `KernelMatrix`, a diverse library of kernel functions, as well as meta classes like `KernelSet` and `KernelBucket` to manage an array of kernel matrices. In addition, a library of kernel operations and related utilities are included.\n",
    "\n",
    "\n",
    "## Table of Contents <a name=\"toc\"></a>\n",
    "- [Kernel functions](#kerfuncs)\n",
    "- [Kernel matrix](#kernelmatrix)\n",
    "- [Attributes for kernel matrix](#attr_km)\n",
    "- [Containers](#kmcollections)\n",
    "- [Usage in kernel machines](#usage_kernel_machines)\n",
    "- [Drop-in Estimator classes](#kernelmachine)\n",
    "- [Advanced applications](#advanced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some imports and setup out of our way for a smooth operation of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import warnings\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel functions <a name=\"kerfuncs\"></a> \n",
    "\n",
    "Let's get started with some **kernel functions**!\n",
    "\n",
    "A [kernel function](https://en.wikipedia.org/wiki/Positive-definite_kernel) takes in two samples (each represented by an array of values in their raw input space) as inputs and computes their inner product. In a typical machine learning library, these kernel functions are usually directly implemented as mathematical formulas, that blindly compute and return the inner product. They are almost always without any structure, validation or representation associated with them, which can be a recipe for invalid or disastrous implementations. In this library, given kernel functions are key to and at the core of everything, we take a more concerted effort to enforce certain structure, uniform validation and readable representation. We achieve this by defining a `BaseKernelFunction` abstract base class and making each kernel function inherit from it.\n",
    "\n",
    "The `BaseKernelFunction` enforces each derived kernel:\n",
    "1. to be callable, with two inputs\n",
    "2. to have a name and a str representation\n",
    "3. provides a method to check whether the derived kernel func is a valid kernel i.e. the kernel matrix derived on a random sample is positive semi-definite (PSD)\n",
    "4. and that it is symmetric (via tests) as required.\n",
    "\n",
    "These properties can be verified using the built-in kernel functions such as `PolyKernel` and `GaussianKernel` e.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polynomial(degree=4,b=0)\n",
      "gaussian(sigma=2.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gaussian(sigma=2.0)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kernelmethods import PolyKernel, GaussianKernel, LinearKernel\n",
    "\n",
    "poly = PolyKernel(degree=4)\n",
    "rbf = GaussianKernel()\n",
    "# you can print/present then in many ways\n",
    "print(poly)\n",
    "print(rbf)\n",
    "repr(rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these are \"children\" of `BaseKernelFunction`, and hence have the aforementioned desirable properties and relevant attributes such as *degree* and intercept (*b*), a name and validation on input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 0, 'polynomial']\n",
      "polynomial(degree=4,b=0)\n"
     ]
    }
   ],
   "source": [
    "print([poly.degree, poly.b, poly.name])\n",
    "# which can also be conveniently presented via its repr or str form\n",
    "print(poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can be called with two vectorial inputs, which returns their input product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = np.array([2, 3, 4])\n",
    "poly(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an internal input validation - which throws a `ValueError` if the data is not of the right type or otherwise invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-7c24f8b6cc80>\", line 2, in <module>\n",
      "    poly([1, 2, 3], [4, 5, 'a'])\n",
      "  File \"/Users/Reddy/dev/kernelmethods/kernelmethods/numeric_kernels.py\", line 46, in __call__\n",
      "    x, y = check_input_arrays(x, y, ensure_dtype=np.number)\n",
      "  File \"/Users/Reddy/dev/kernelmethods/kernelmethods/utils.py\", line 31, in check_input_arrays\n",
      "    y = ensure_ndarray_1D(y, ensure_dtype)\n",
      "  File \"/Users/Reddy/dev/kernelmethods/kernelmethods/utils.py\", line 62, in ensure_ndarray_1D\n",
      "    return ensure_ndarray_size(array, ensure_dtype=ensure_dtype, ensure_num_dim=1)\n",
      "  File \"/Users/Reddy/dev/kernelmethods/kernelmethods/utils.py\", line 78, in ensure_ndarray_size\n",
      "    ''.format(array.dtype, ensure_dtype))\n",
      "ValueError: input data type <U21 is not compatible with the required <class 'numpy.number'>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    poly([1, 2, 3], [4, 5, 'a'])\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More importantly, every kernel function has a method to verify that kernel function is valid (KM induced is PSD!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly.is_psd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using the pre-defined classes, one can easily build new classes either by defining new classes themselves (starting from `BaseKernelFunction` or its derived classes), or by simply specifying a callable and using the `KernelFromCallable` convenience class. For example, you a new type of polynomial kernel without an intercept, that can be achieved via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly_no_intercept\n"
     ]
    }
   ],
   "source": [
    "from kernelmethods.base import BaseKernelFunction, KernelFromCallable\n",
    "\n",
    "# define that function\n",
    "def poly_no_intercept(x, y, degree=2):\n",
    "    return x.dot(y.T) ** degree\n",
    "\n",
    "new_poly = KernelFromCallable(input_func=poly_no_intercept)\n",
    "print(new_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that `new_poly` is indeed a KernelFunction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(new_poly, BaseKernelFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can check its properties and usability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_poly(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also quickly check if this new function is a valid [mercer kenel](https://en.wikipedia.org/wiki/Mercer%27s_theorem) or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_poly.is_psd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you will see the rbf is also pSD\n",
    "rbf.is_psd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel matrix <a name=\"kernelmatrix\"></a>\n",
    "\n",
    "The gram matrix resulting from the pairwise application of the kernel function results in what is called the kernel matrix. This is a key data structure for all the kernel methods and learning algorithms. We designed `KernelMatrix` to make it self-contained, efficient and yet generic. \n",
    "\n",
    "You can import it simply by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from kernelmethods import KernelMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance can be created by specifying which function to be used as the kernel, and an optional name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "km = KernelMatrix(rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect its properties easily, and get an easy to read representation anytime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelMatrix: gaussian(sigma=2.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying a kernel function is not enough - we usually need to attach and apply it to a sample. Let's create a simple dataset consisting of 10 points with 4 features each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28 0.82 0.66 0.43]\n",
      " [0.58 0.43 0.7  0.32]\n",
      " [0.91 0.41 0.79 0.77]\n",
      " [0.   0.32 1.   0.82]\n",
      " [0.85 0.53 0.37 0.76]\n",
      " [0.91 0.15 0.13 0.71]\n",
      " [0.43 0.9  0.51 0.81]\n",
      " [0.15 0.25 0.43 0.52]\n",
      " [0.05 0.15 0.63 0.97]\n",
      " [0.07 0.25 0.08 0.57]]\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.rand(10, 4)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attaching it is as easy as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "km.attach_to(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then see it is ready for use, with a clear repr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelMatrix: gaussian(sigma=2.0) (normed=True) on sample (10, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display the full matrix simply with the `.full` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.97, 0.92, 0.93, 0.93, 0.86, 0.98, 0.95, 0.91, 0.91],\n",
       "       [0.97, 1.  , 0.96, 0.92, 0.95, 0.92, 0.94, 0.96, 0.91, 0.91],\n",
       "       [0.92, 0.96, 1.  , 0.9 , 0.98, 0.94, 0.93, 0.91, 0.9 , 0.85],\n",
       "       [0.93, 0.92, 0.9 , 1.  , 0.87, 0.82, 0.91, 0.95, 0.98, 0.89],\n",
       "       [0.93, 0.95, 0.98, 0.87, 1.  , 0.97, 0.96, 0.92, 0.89, 0.9 ],\n",
       "       [0.86, 0.92, 0.94, 0.82, 0.97, 1.  , 0.89, 0.91, 0.88, 0.91],\n",
       "       [0.98, 0.94, 0.93, 0.91, 0.96, 0.89, 1.  , 0.93, 0.91, 0.91],\n",
       "       [0.95, 0.96, 0.91, 0.95, 0.92, 0.91, 0.93, 1.  , 0.97, 0.98],\n",
       "       [0.91, 0.91, 0.9 , 0.98, 0.89, 0.88, 0.91, 0.97, 1.  , 0.94],\n",
       "       [0.91, 0.91, 0.85, 0.89, 0.9 , 0.91, 0.91, 0.98, 0.94, 1.  ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the kernel matrices are `normalized` by default, and hence all the diagonal elements are `1.0`. If you have a specific requirement not to normalize it, you can choose `normalized=False` during instantiation.\n",
    "\n",
    "which then allows retrieval of various properties including size, full kernel matrix or portions of it in a convenient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 100, \n",
      "\tsamples: 10 \n",
      "shape of KM:  (10, 10)\n"
     ]
    }
   ],
   "source": [
    "print('number of elements: {}, \\n\\tsamples: {} '.format(km.size, km.num_samples))\n",
    "print('shape of KM: ', km.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as easily evaluate its frobenius norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.309624805148443"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.frob_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, this class offers the following public attributes of KM (including methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attach_to',\n",
       " 'attributes',\n",
       " 'center',\n",
       " 'centered',\n",
       " 'diagonal',\n",
       " 'frob_norm',\n",
       " 'full',\n",
       " 'full_sparse',\n",
       " 'get_attr',\n",
       " 'kernel',\n",
       " 'name',\n",
       " 'normalize',\n",
       " 'normed_km',\n",
       " 'num_samples',\n",
       " 'set_attr',\n",
       " 'shape',\n",
       " 'size']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ a for a in dir(km)  if not a.startswith('_') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform common operations such as centering, and normalization quite easily, via `km.center()` and  `km.normalize()`, as well as query its diagonal with `km.diagonal()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage in kernel machines <a name='usage_kernel_machines'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a kernel matrix is computed, you could pass it on to any kernel learning algorithms in place of the original sample. For example, to `SVC` or `KernelRidge` in `scikit-learn` by specifying the kernel=`precomputed` anf supplying the `km.full` instead of the `X`. Let's generate a toy sample first to compute a kernel matrix first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelMatrix: gaussian(sigma=2.0) (normed=True) on sample (100, 20)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "sample_data, labels = make_classification()\n",
    "\n",
    "rbf = GaussianKernel()\n",
    "toy_km = KernelMatrix(rbf)\n",
    "toy_km.attach_to(sample_data)\n",
    "toy_km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, prepare the kernel machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='precomputed', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='precomputed')\n",
    "svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, training the kernel machine is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_km.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toy_km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseSVC.decision_function of SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='precomputed', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(toy_km.full, labels)\n",
    "svm.decision_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the resulting classifier is as good as anything else. \n",
    "\n",
    "As the input to svm `.fit()` was a precomputed kernel matrix, not original samples, we now need to provide a kernel matrix that is a dot product between train and test sets! This can easily be achieved by attaching these two samples to the same instance. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_data, test_labels = make_classification(n_samples=40, n_features=20)\n",
    "\n",
    "new_km = KernelMatrix(rbf)\n",
    "new_km.attach_to(sample_one=test_data, sample_two=sample_data) # notice test_set goes first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `new_km.full` has the full kernel matrix from the application of kernel function, with pair-wise dot products between points across the two samples - the first one with 40 points, and the second one with 100 points, each in the 20-dimensional space. You can verify that is indeed the case with the shape of the new kernel matrix! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_km.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can make predictions on the test set easily via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = svm.predict(new_km.full)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can check the performance of this toy exercise on randomly generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  6],\n",
       "       [12,  8]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_labels, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which shows the performance is horrible - but we were expecting this to be only a proof of concept exercise!\n",
    "\n",
    "[Go back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes for KernelMatrix <a name=\"attr_km\"></a>\n",
    "\n",
    "Another cool feature of the `KernelMatrix` class is the ability to attach arbitary user-defined attributes, for easy identification of a given kernel matrix. This is especially handy when one has to traverse among a large collection of kernel matrices, and programmatic identification is necessary!\n",
    "\n",
    "For example, you can identify the KM with the source of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "km.set_attr('source', 'random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or anything else you wish, like properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "km.set_attr('properties', ['sigma', 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily retrieve all the properties with `.attributes()` which returns the internal dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'random', 'properties': ['sigma', 4]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or just the one you like via `.get_attr()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'random'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.get_attr('source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility of these will be more obvious when dealing with container classes like [`KernelBucket` and `KernelSet`](#kmcollections)\n",
    "\n",
    "[Go back to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Container classes: KernelSet and KernelBucket <a name=\"kmcollections\"></a>\n",
    "\n",
    "When dealing multiple kernel matrices e.g. as part of multiple kernel learning (MKL), a number of validation and sanity checks need to be performed. Some of these checks include ensuring compatible size of the kernel matrices (KMs), as well as knowing these matrices are generated from the same sample. We refer to such collection of KMs a `KernelSet`. Moreover, accessing a subset of these KMs e.g. filtered by some metric is often necessary while trying to optimize algorithms like MKL. To serve as candidates for optimization, it is often necessary to *sample* and generate a large number of KMs, here referred to as a *bucket*. The ``KernelSet`` and ``KernelBucket`` make these tasks easy and extensible while keeping their rich annotations and structure (meta-data etc).\n",
    "\n",
    "Let's take a look at their utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kernelmethods import KernelSet, KernelBucket\n",
    "\n",
    "# let's build 3 kernel matrices\n",
    "rbf = KernelMatrix(GaussianKernel(sigma=10))\n",
    "lin = KernelMatrix(LinearKernel())\n",
    "poly = KernelMatrix(PolyKernel(degree=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have 3 KMs (that are not attached to any samplet yet), we can collect them together with a simple `KernelSet` instantiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelSet(3 kernels, None samples):\n",
       "\tKernelMatrix: linear\n",
       "\tKernelMatrix: polynomial(degree=2,b=0)\n",
       "\tKernelMatrix: gaussian(sigma=10.0) "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kset = KernelSet()\n",
    "kset.append(lin)\n",
    "kset.append(poly)\n",
    "kset.append(rbf)\n",
    "kset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, this can also be achieved with a single call with a list input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelSet(3 kernels, None samples):\n",
       "\tKernelMatrix: linear\n",
       "\tKernelMatrix: polynomial(degree=2,b=0)\n",
       "\tKernelMatrix: gaussian(sigma=10.0) "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kset = KernelSet([lin, poly, rbf])\n",
    "kset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can see that its string repr is clearly informing of its internal structure. Once we have a such kernel set, we can easily attach a sample to all of them at once with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelSet(3 kernels, 100 samples):\n",
       "\tKernelMatrix: linear (normed=True) on sample (100, 20)\n",
       "\tKernelMatrix: polynomial(degree=2,b=0) (normed=True) on sample (100, 20)\n",
       "\tKernelMatrix: gaussian(sigma=10.0) (normed=True) on sample (100, 20) "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kset.attach_to(sample_data)\n",
    "kset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that sample is attached to each and every kernel matrix inside, and the compatibility for being in the set is determined by number of samples (in this case 100). So if we were to try attaching an incompatible KM, it would result in an error. This error is specifically idenfied as `KMSetAdditionError`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-38-03a7e559bba5>\", line 5, in <module>\n",
      "    kset.append(poly2)\n",
      "  File \"/Users/Reddy/dev/kernelmethods/kernelmethods/base.py\", line 1025, in append\n",
      "    ''.format(KM.num_samples, self.num_samples))\n",
      "kernelmethods.config.KMSetAdditionError: Dimension of this KM 60 is incompatible with KMSet of 100! \n"
     ]
    }
   ],
   "source": [
    "sample_data_n60 = np.random.rand(60, 10)\n",
    "poly2 = KernelMatrix(PolyKernel())\n",
    "poly2.attach_to(sample_data_n60)\n",
    "try:\n",
    "    kset.append(poly2)\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such management of a collection of KMs helps you gain confidence in their usage, instead of constantly repeating validation checks all over the place and still be unable to trust the implementation!\n",
    "\n",
    "Once you constructed it, you can access a single element directly with `[]`, or iterate through them quite easily as with any Iterable in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KernelMatrix: polynomial(degree=2,b=0) (normed=True) on sample (100, 20)\n",
      "KernelMatrix: linear (normed=True) on sample (100, 20)\n",
      "Iterating through ..\n",
      "KernelMatrix: linear (normed=True) on sample (100, 20)\n",
      "KernelMatrix: polynomial(degree=2,b=0) (normed=True) on sample (100, 20)\n",
      "KernelMatrix: gaussian(sigma=10.0) (normed=True) on sample (100, 20)\n"
     ]
    }
   ],
   "source": [
    "print(kset[1])\n",
    "print(kset[0])\n",
    "print('Iterating through ..')\n",
    "for km in kset:\n",
    "    print(km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access some elements of the set is quite easy with the `.take()` method. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subset(2 kernels, 100 samples):\n",
       "\tKernelMatrix: polynomial(degree=2,b=0) (normed=True) on sample (100, 20)\n",
       "\tKernelMatrix: gaussian(sigma=10.0) (normed=True) on sample (100, 20) "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = kset.take([1,2], name='subset')\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily find the number of KMs in the set with `.size` attribute and the common number of samples with `.num_samples` attribute.\n",
    "\n",
    "Also, KernelSet lets you apply a single attribute to its collections, making it convenient for programmatic comparison or query later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kset.set_attr('source', 'magic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KernelBucket\n",
    "\n",
    "`KernelBucket` is a child class of `KernelSet` that helps populate the set with a chosen range of parameter values for different kernel functions. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelBucket(13 kernels, None samples):\n",
       "\tKernelMatrix: linear\n",
       "\tKernelMatrix: polynomial(degree=2,b=0)\n",
       "\tKernelMatrix: polynomial(degree=3,b=0)\n",
       "\tKernelMatrix: polynomial(degree=4,b=0)\n",
       "\tKernelMatrix: gaussian(sigma=0.03125)\n",
       "\tKernelMatrix: gaussian(sigma=0.125)\n",
       "\tKernelMatrix: gaussian(sigma=0.5)\n",
       "\tKernelMatrix: gaussian(sigma=2.0)\n",
       "\tKernelMatrix: gaussian(sigma=8.0)\n",
       "\tKernelMatrix: gaussian(sigma=32.0)\n",
       "\tKernelMatrix: laplacian(gamma=2)\n",
       "\tKernelMatrix: laplacian(gamma=8)\n",
       "\tKernelMatrix: laplacian(gamma=32) "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb = KernelBucket()\n",
    "kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, KB adds 3 different types of kernel functions each with a range of their core parameter values. And this bucket would have the same behaviours and properties KernelSet (iteration, attributes, append, access etc). You can also choose to supply your own value ranges e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "high_sigma_bucket(4 kernels, None samples):\n",
       "\tKernelMatrix: linear\n",
       "\tKernelMatrix: gaussian(sigma=100.0)\n",
       "\tKernelMatrix: gaussian(sigma=400.0)\n",
       "\tKernelMatrix: gaussian(sigma=3435.0) "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb = KernelBucket(rbf_sigma_values=[100, 400, 3435], \n",
    "                  laplacian_gamma_values=None,\n",
    "                  poly_degree_values=None,\n",
    "                  name='high_sigma_bucket')\n",
    "kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library also provides a convenient `make_kernel_bucket` function to populate a bucket with either `exhaustive` or `light` ranges of parameter value tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KBucketLight(7 kernels, None samples):\n",
       "\tKernelMatrix: linear\n",
       "\tKernelMatrix: polynomial(degree=2,b=0)\n",
       "\tKernelMatrix: polynomial(degree=3,b=0)\n",
       "\tKernelMatrix: gaussian(sigma=0.125)\n",
       "\tKernelMatrix: gaussian(sigma=0.5)\n",
       "\tKernelMatrix: gaussian(sigma=2.0)\n",
       "\tKernelMatrix: laplacian(gamma=2) "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kernelmethods.sampling import make_kernel_bucket\n",
    "kbl = make_kernel_bucket(strategy='light')\n",
    "kbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where you can see a different, but fewer/lighter set of values for parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Go back to table of contents](#toc)\n",
    "\n",
    "## Drop-in Estimator Classes via `KernelMachine` <a name=\"kernelmachine\"></a>\n",
    "\n",
    "Besides being able to use the aforementioned `KernelMatrix` in SVM or another kernel machine, this library makes life even easier by providing drop-in Estimator classes directly. It's called `KernelMachine` and they can be dropped in place of `sklearn.svm.SVC` anywhere. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelMachine(k_func=gaussian(sigma=2.0), learner_id='SVR')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kernelmethods import KernelMachine\n",
    "km = KernelMachine(k_func=GaussianKernel())\n",
    "km.fit(X=sample_data, y=labels)\n",
    "km\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And making predictions on new samples is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9 0.9 0.1 0.1 0.1 0.1 0.1 0.9 0.9 0.9 0.1 0.1 0.1 0.9 0.9 0.9 0.1 0.9\n",
      " 0.9 0.1 0.9 0.1 0.1 0.9 0.9 0.1 0.1 0.1 0.1 0.1 0.9 0.1 0.9 0.1 0.9 0.1\n",
      " 0.9 0.1 0.9 0.1 0.9 0.1 0.1 0.9 0.9 0.9 0.9 0.1 0.1 0.1 0.9 0.9 0.1 0.1\n",
      " 0.1 0.9 0.9 0.9 0.9 0.1 0.1 0.9 0.1 0.9 0.1 0.9 0.9 0.9 0.1 0.9 0.1 0.9\n",
      " 0.1 0.1 0.9 0.9 0.1 0.9 0.9 0.1 0.1 0.1 0.1 0.9 0.1 0.1 0.1 0.1 0.9 0.1\n",
      " 0.1 0.9 0.9 0.9 0.9 0.1 0.9 0.1 0.1 0.9]\n"
     ]
    }
   ],
   "source": [
    "predicted_y = km.predict(sample_data)\n",
    "print(predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And if you're not sure which kernel function is optimal for your dataset, you can employ `OptimalKernelSVR`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Reddy/anaconda/envs/py36/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/Reddy/anaconda/envs/py36/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/Reddy/anaconda/envs/py36/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/Reddy/anaconda/envs/py36/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/Reddy/anaconda/envs/py36/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/Reddy/anaconda/envs/py36/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/Reddy/anaconda/envs/py36/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimalKernelSVR(k_bucket=KBucketExhaustive(13 kernels, 100 samples):\n",
      "\tKernelMatrix: linear (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: polynomial(degree=2,b=0) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: polynomial(degree=3,b=0) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: polynomial(degree=4,b=0) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: gaussian(sigma=0.03125) (n...\n",
      "\tKernelMatrix: gaussian(sigma=2.0) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: gaussian(sigma=8.0) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: gaussian(sigma=32.0) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: laplacian(gamma=2) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: laplacian(gamma=8) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: laplacian(gamma=32) (normed=True) on sample (100, 20) )\n"
     ]
    }
   ],
   "source": [
    "from kernelmethods import OptimalKernelSVR\n",
    "opt_km = OptimalKernelSVR('exhaustive')\n",
    "opt_km.fit(X=sample_data, y=labels)\n",
    "print(opt_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result of `print()` you can clearly see that `OptimalKernelSVR` is indeed built on `KBucketExhaustive` which ranked 13 kernels (and detailed info such as parameters and values) to choose the best for this particular sample.\n",
    "\n",
    "You can also easily query which kernel performed best for this sample via the `.opt_kernel` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelMatrix: linear (normed=True) on sample (100, 20)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_km.opt_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which seems to be a `linear` kernel function! Occam's razor, ha!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily make predictions on the a new sample, and see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93  0.96 -0.06 -0.07 -0.07  0.04  0.12  0.62  0.77  1.1   0.27  0.21\n",
      "  0.1   0.12  0.91  1.24  0.14  0.86  0.08 -0.29  0.4   0.29  0.21  0.8\n",
      "  0.9  -0.12  0.1   0.23  0.1   0.1   0.85  0.07  0.73  0.12  0.68 -0.27\n",
      "  0.65  0.54  1.1   0.1   0.81  0.7  -0.15  0.9   1.55  1.04  0.9  -0.01\n",
      "  0.37  0.1   1.22  0.83 -0.36  0.06  0.26  1.13  1.    0.68  0.9  -0.1\n",
      "  0.16  0.76  0.18  0.98  0.04  0.21  0.95  0.75  0.3   0.9   0.05  1.1\n",
      "  0.24  0.03  0.9   0.94  0.21  0.41  0.94  0.18 -0.1   0.29 -0.55  0.9\n",
      "  0.1   0.    0.02  0.45  1.   -0.01  0.1   1.02  0.91  0.75  0.75  0.28\n",
      "  0.67  0.13  0.35  0.9 ]\n"
     ]
    }
   ],
   "source": [
    "predicted_y = opt_km.predict(sample_data)\n",
    "print(predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to table of contents](#toc)\n",
    "\n",
    "## Advanced applications (MKL etc) <a name=\"advanced\"></a>\n",
    "\n",
    "Now that you have a reasonable grasp of the various functions of this library and how to use them, I'd like to present one quick example of how to use this machinery for an advanced application like Multiple Kernel Learning (MKL). Briefly, the technique of MKL boils down to a weighted linear combination of kernel matrices. Most of varitations in different MKL techniques are due to the way these weights are computed (optimization) and pruned (sparsity), in trying to optimize the performance of a kernel machine (often an SVM or SVR) for a given sample. \n",
    "\n",
    "If you have a kernel set with `n` KMs whose are weights are in `weight_vec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KBucketLight(7 kernels, 100 samples):\n",
      "\tKernelMatrix: linear (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: polynomial(degree=2,b=0) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: polynomial(degree=3,b=0) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: gaussian(sigma=0.125) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: gaussian(sigma=0.5) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: gaussian(sigma=2.0) (normed=True) on sample (100, 20)\n",
      "\tKernelMatrix: laplacian(gamma=2) (normed=True) on sample (100, 20) \n",
      "[0.09 0.03 0.17 0.25 0.19 0.17 0.12]\n"
     ]
    }
   ],
   "source": [
    "kb_light = make_kernel_bucket(strategy='light')\n",
    "sample_data, labels = make_classification()\n",
    "kb_light.attach_to(sample_data)\n",
    "print(kb_light)\n",
    "\n",
    "weight_vec = np.random.rand(kb_light.size) # toy example\n",
    "weight_vec /= weight_vec.sum() # normalizing them!\n",
    "print(weight_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the corresponding composite kernel matrix can computed easily with either the `linear_combination` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.04,  0.07, ..., -0.03, -0.02,  0.07],\n",
       "       [ 0.04,  1.  ,  0.01, ..., -0.02,  0.05,  0.01],\n",
       "       [ 0.07,  0.01,  1.  , ..., -0.01, -0.03,  0.02],\n",
       "       ...,\n",
       "       [-0.03, -0.02, -0.01, ...,  1.  , -0.  , -0.  ],\n",
       "       [-0.02,  0.05, -0.03, ..., -0.  ,  1.  ,  0.02],\n",
       "       [ 0.07,  0.01,  0.02, ..., -0.  ,  0.02,  1.  ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kernelmethods.operations import linear_combination\n",
    "lin_comb = linear_combination(kb_light, weight_vec)\n",
    "lin_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or a special child class of `CompositeKernel` called `WeightedAverageKernel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeightedAverageKernel-->KBucketLight(7 kernels, 100 samples):\n",
       "\tKernelMatrix: linear (normed=True) on sample (100, 20)\n",
       "\tKernelMatrix: polynomial(degree=2,b=0) (normed=True) on sample (100, 20)\n",
       "\tKernelMatrix: polynomial(degree=3,b=0) (normed=True) on sample (100, 20)\n",
       "\tKernelMatrix: gaussian(sigma=0.125) (normed=True) on sample (100, 20)\n",
       "\tKernelMatrix: gaussian(sigma=0.5) (normed=True) on sample (100, 20)\n",
       "\tKernelMatrix: gaussian(sigma=2.0) (normed=True) on sample (100, 20)\n",
       "\tKernelMatrix: laplacian(gamma=2) (normed=True) on sample (100, 20) "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kernelmethods.base import WeightedAverageKernel\n",
    "wak = WeightedAverageKernel(kb_light, weight_vec)\n",
    "wak.fit()\n",
    "wak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from which the KM can be accessed with `wak.composite_KM` or `wak.full`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.04,  0.07, ..., -0.03, -0.02,  0.07],\n",
       "       [ 0.04,  1.  ,  0.01, ..., -0.02,  0.05,  0.01],\n",
       "       [ 0.07,  0.01,  1.  , ..., -0.01, -0.03,  0.02],\n",
       "       ...,\n",
       "       [-0.03, -0.02, -0.01, ...,  1.  , -0.  , -0.  ],\n",
       "       [-0.02,  0.05, -0.03, ..., -0.  ,  1.  ,  0.02],\n",
       "       [ 0.07,  0.01,  0.02, ..., -0.  ,  0.02,  1.  ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wak.composite_KM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that they both return the same kernel matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(wak.composite_KM, lin_comb).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composite kernels can be produced in many other ways e.g. using `AverageKernel` or `SumKernel` etc. \n",
    "\n",
    "Once you have a desired composite kernel, MKL machine can easily be constructed by passing it as a precomputed kernel to any other toolbox e.g. an Estimator in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='precomputed', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkl = SVC(kernel='precomputed')\n",
    "mkl.fit(X=wak.composite_KM, y=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it :).\n",
    "\n",
    "Weights for different kernel matrices can also be computed via their *kernel target alignment* to the target values `y`. The `operations` and `ranking` submodules provide different metrics for this purpose such as `centered_alignment` and performance in cross-validation (CV) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Stay tuned for more tutorials, examples and comprehensive docs.\n",
    "\n",
    "[Go back to table of contents](#toc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
